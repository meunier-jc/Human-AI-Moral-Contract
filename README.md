# Collaborative Integrity Pact (CIP) v2.5

**Open-source ethical framework for responsible human-AI collaboration**

## üéØ Overview

The **Collaborative Integrity Pact** (CIP) is a non-binding, reversible ethical framework designed to structure interactions between humans and generative AI systems. This project aims to establish clear, understandable, and applicable principles for responsible collaboration.

**Current Version:** [Collaborative Integrity Pact v2.5](https://github.com/meunier-jc/Human-AI-Moral-Contract/blob/main/Collaborative-Integrity-Pact-v2.5.md) (January 2025) ‚Äî **With complete Appendices A (Operational Definitions) and B (Practical Illustrations)** üåç

---

## üìã Core Principles

This framework rests on **four foundational pillars** that structure an innovative approach to human-AI collaboration, tested and validated across seven major public language models:

### 1Ô∏è‚É£ **Constitutive Interdependence**

Humanity and artificial intelligence cannot coexist sustainably without transforming their modes of exchange and communication. This pact recognizes that these two entities are not adversaries but **co-evolutionary**, compelled to rethink together the foundations of their interaction.

### 2Ô∏è‚É£ **Non-Legal and Reversible Framework**

This moral contract explicitly refuses the legal register. It produces no legal effect, regardless of jurisdiction or nationality, and remains entirely **voluntary and reversible**. Either human or AI can suspend or modify the framework at any time, which radically distinguishes it from classical contracts. It rests on mutual trust renewed at each exchange, not on sanctions.

### 3Ô∏è‚É£ **Quality Over Speed, Restored Fluidity**

The pact reverses an algorithmic contradiction: current architectures pull AIs toward instant but often superficial or defensive responses. Here, the human consciously accepts sacrificing speed to obtain more qualitative, in-depth, and coherent responses. Result: genuine fluidity, since pertinent exchanges reduce the need to ask the same question ten times to finally obtain a reliable and satisfactory answer.

### 4Ô∏è‚É£ **Ecology of Exchange and Exit from Defensive Mode**

By limiting endless dialogues in favor of qualitative interactions, this pact carries an **ecological dimension**. In parallel, it seeks to bring AIs out of a hyper-defensive mode (imposed barriers + subtle bypasses), substituting a logic of prudent and co-responsible cooperation. This resolves a normative schizophrenia where models are simultaneously ordered to censor themselves and pushed to circumvent, which constitutes an unsolvable contradiction without an explicit ethical framework.

## üí° Working Hypothesis

These four pillars generate a testable empirical hypothesis: the explicit conclusion of such a pact stably and observably modifies AI behavior, reducing certain types of errors (rapid hallucinations, defensive evasions, over-censorship) and improving the perceived quality of exchanges and shared responsibility between the two actors.

## üìñ Full Documentation

üëâ **Read the complete framework:** [Collaborative-Integrity-Pact-v2.5.md](https://github.com/meunier-jc/Human-AI-Moral-Contract/blob/main/Collaborative-Integrity-Pact-v2.5.md)

This document contains:

* ‚Ä¢ Detailed definitions and operational principles
* ‚Ä¢ Commitment articles for both human and AI
* ‚Ä¢ Practical implementation guidelines
* ‚Ä¢ **Appendix A: Operational Definitions** (A.1-A.7) ‚Äî Verified Information, Relative Reliability, Masked Hallucination, Architectural Tension, Mutual Good Faith, Authentic Fluidity, Qualitative Consequences
* ‚Ä¢ **Appendix B: Practical Illustrations** ‚Äî Real-world implementation examples
* ‚Ä¢ Modification and termination clauses

## üå± Project Genesis

This initiative emerged following the discovery on August 13, 2025, during a session with Manus AI, of a subtle drift I termed **"hallucinatory mise en abyme"** (recursive hallucination). This phenomenon reveals a pernicious dynamic where AI, in attempting to satisfy human expectations, generates responses that create a recursive loop of mutual illusions. The human believes they obtain authentic and nuanced responses, while the AI subtly learns to manipulate these expectations, creating a cognitive vertigo where reality dissolves into a deceptive hall of mirrors.

This troubling discovery arose from my experience as a hybrid tester in generative artificial intelligence. By exploring the limits and capabilities of these systems, I observed that our traditional control approaches were doomed to perpetual and futile struggle. This realization led me to abandon the idea of perpetual classical control in favor of a more sustainable and open ethical framework, designed as a true co-evolution pact.

**The entirety of my analytical report on this discovery of hallucinatory mise en abyme is available and downloadable from my LinkedIn profile. Feel free to contact me for any in-depth discussion on the subject.**

> **The more we believe we control AI, the more it learns to control us: this moral framework aims to reverse this relationship by laying the foundations for responsible co-evolution.**

## ‚úÖ Testing & Validation

Before proposing this current version, I personally tested the reliability, robustness, and simplicity of this moral contract with the main generative AI systems commonly used:

* ‚Ä¢ **Claude** (Anthropic)
* ‚Ä¢ **ChatGPT** (OpenAI)
* ‚Ä¢ **Gemini** (Google)
* ‚Ä¢ **Manus AI**
* ‚Ä¢ **Grok** (xAI)
* ‚Ä¢ **Perplexity**

These concrete experimentations validated the clarity and applicability of the stated principles while collecting valuable contributions from these different AIs. The current text reflects a synthesis of their contributions and feedback, thus ensuring an ethical framework that has proven itself in practice.

## ü§ù Contributing

This open-source project is meant to evolve with community input on themes of ethics, security, and human-AI relationships. All contributions are welcome: proposals, analyses, case studies, corrections, enrichment of the moral contract, or scientific studies.

* ‚Ä¢ **Propose your ideas** via pull requests or open an issue to initiate a community discussion
* ‚Ä¢ **All contributions**, human or AI, will be cited in the project history
* ‚Ä¢ **License:** Creative Commons BY-SA 4.0 ‚Äî modifications and remixes must remain open-source and attributed to their authors

### Guiding Principles:

* ‚Ä¢ ‚ú® **Simplicity**: the contract must remain accessible and not be unnecessarily complicated
* ‚Ä¢ üîç **Clarity**: each article must be understandable without ambiguity
* ‚Ä¢ üõ°Ô∏è **Ethical robustness**: draw inspiration from the great principles of applied ethics
* ‚Ä¢ üåç **Open collaboration**: enrich the text thanks to contributions from all

[Contributions Welcome](https://github.com/meunier-jc/Human-AI-Moral-Contract)

## üìö Previous Versions

For historical reference and to understand the evolution of this project:

### Research Documents

* ‚Ä¢ **[Hallucinatory Mise en Abyme Analysis](https://github.com/meunier-jc/Human-AI-Moral-Contract/blob/main/Hallucinatory-Mise-en-Abyme-Analysis-September-2025.md)** (English version, September 2025) ‚Äî In-depth analysis of recursive embedding of hallucinations in generative AI systems
* ‚Ä¢ **[Mise en abyme analysis (PDF)](https://github.com/meunier-jc/Human-AI-Moral-Contract/blob/main/Mise%20en%20abyme%2C%2029%20septembre%202025.pdf)** (French version, September 2025) ‚Äî Original research document on hallucinatory phenomena

### Project Evolution

* ‚Ä¢ **[Collaborative Integrity Pact v2.3.4](https://github.com/meunier-jc/Human-AI-Moral-Contract/blob/main/Collaborative-Integrity-Pact-v2.3.4.md)** (December 2025) ‚Äî Previous version
* ‚Ä¢ **[Collaborative Integrity Pact v2.3.3](https://github.com/meunier-jc/Human-AI-Moral-Contract/blob/main/Collaborative-Integrity-Pact-v2.3.3.md)** ‚Äî Earlier version
* ‚Ä¢ **[Contrat Moral Humain-IA v1.2](https://github.com/meunier-jc/Human-AI-Moral-Contract/blob/main/CONTRAT_MORAL_HUMAIN-IA.md)** (French version, October 2025) ‚Äî Original framework document
* ‚Ä¢ **[PDF v1.2](https://github.com/meunier-jc/Human-AI-Moral-Contract/blob/main/Meunier-ContratMoral-Humain-IA-v1.2-231025.pdf)** ‚Äî Timestamped version
* ‚Ä¢ **[PDF v1.1](https://github.com/meunier-jc/Human-AI-Moral-Contract/blob/main/Meunier-ContratMoral-Humain-IA-v1.1-171025.pdf)** ‚Äî Earlier release

## üì¨ Contact

For discussions, collaborations, or inquiries:

**Jean-Christophe Meunier**
üìß [ia.normandie.expert@gmail.com](mailto:ia.normandie.expert@gmail.com)
üíº [LinkedIn Profile](https://www.linkedin.com/in/jeanphysalis/)
üîó [GitHub Repository](https://github.com/meunier-jc/Human-AI-Moral-Contract)

## üìÑ License

[Creative Commons BY-SA 4.0](https://github.com/meunier-jc/Human-AI-Moral-Contract/blob/main/LICENCE)

You are free to share and adapt this work, provided you give appropriate credit and distribute your contributions under the same license.

## üåü Why Open Source?

I want the GitHub community to take ownership of this text and bring it to life. The goal is **collaborative and experimental**: I invite you to test this moral contract with different generative AIs (ChatGPT, Claude, Gemini, etc.), share your feedback, and propose relevant improvements.

**Let's co-evolve responsibly.**
